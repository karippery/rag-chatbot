import logging
import time
from typing import Optional, Tuple
from django.conf import settings
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch

logger = logging.getLogger(__name__)


class LLMService:
    # Both models are preloaded at startup and kept in memory
    MODEL_REGISTRY = {
        "Qwen/Qwen2-0.5B-Instruct":   None,  # quick
        "Qwen/Qwen2.5-1.5B-Instruct": None,  # detailed
    }
    TOKENIZER_REGISTRY = {
        "Qwen/Qwen2-0.5B-Instruct":   None,
        "Qwen/Qwen2.5-1.5B-Instruct": None,
    }
    PIPELINE_REGISTRY = {
        "Qwen/Qwen2-0.5B-Instruct":   None,
        "Qwen/Qwen2.5-1.5B-Instruct": None,
    }

    DEFAULT_MODEL = "Qwen/Qwen2-0.5B-Instruct"

    def load_model(self, model_name: Optional[str] = None) -> bool:
        model_name = model_name or self.DEFAULT_MODEL

        if model_name not in self.MODEL_REGISTRY:
            logger.warning(f"Unknown model requested: {model_name}. Using default.")
            model_name = self.DEFAULT_MODEL

        if self.PIPELINE_REGISTRY[model_name] is not None:
            return True  # already loaded

        try:
            logger.info(f"Loading LLM model: {model_name}")
            start_time = time.time()

            tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token

            use_gpu = torch.cuda.is_available()

            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                dtype=torch.float16 if use_gpu else torch.float32,
                device_map="auto" if use_gpu else None,
                trust_remote_code=True,
                low_cpu_mem_usage=True,
            )

            if not use_gpu:
                model = model.to("cpu")

            pipe = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer,
                device=0 if use_gpu else -1,
                max_new_tokens=256,
                temperature=0.7,
                top_p=0.95,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
            )

            self.TOKENIZER_REGISTRY[model_name] = tokenizer
            self.MODEL_REGISTRY[model_name] = model
            self.PIPELINE_REGISTRY[model_name] = pipe

            logger.info(
                f"Model '{model_name}' loaded in {time.time() - start_time:.2f}s "
                f"on {'GPU' if use_gpu else 'CPU'}"
            )
            return True

        except Exception as e:
            logger.warning(f"Failed to load model {model_name}: {e}", exc_info=True)
            return False

    def load_all_models(self):
        """Preload all models at startup so no user request pays the cold-start cost."""
        for model_name in self.MODEL_REGISTRY:
            self.load_model(model_name)

    def is_available(self, model_name: Optional[str] = None) -> bool:
        model_name = model_name or self.DEFAULT_MODEL
        if self.PIPELINE_REGISTRY.get(model_name) is not None:
            return True
        return self.load_model(model_name)

    def generate_answer(
        self,
        query: str,
        context: str,
        model_name: Optional[str] = None,
        max_tokens: int = 256,
    ) -> Tuple[str, bool]:
        model_name = model_name or self.DEFAULT_MODEL

        if self.is_available(model_name):
            try:
                return self._generate_with_llm(query, context, model_name, max_tokens)
            except Exception as e:
                logger.warning(f"LLM generation failed: {e}. Falling back to extractive.", exc_info=True)
                return self._generate_extractive(query, context), False

        return self._generate_extractive(query, context), False

    def _generate_with_llm(
        self, query: str, context: str, model_name: str, max_tokens: int
    ) -> Tuple[str, bool]:
        tokenizer = self.TOKENIZER_REGISTRY[model_name]
        pipe = self.PIPELINE_REGISTRY[model_name]

        prompt = self._build_prompt(query, context, tokenizer)
        start_time = time.time()

        result = pipe(
            prompt,
            max_new_tokens=max_tokens,
            temperature=0.7,
            top_p=0.95,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
        )

        if not result or "generated_text" not in result[0]:
            raise ValueError("LLM returned empty or malformed response")

        full_text = result[0]["generated_text"]
        answer = full_text[len(prompt):].strip()

        logger.info(f"Answer generated by '{model_name}' in {time.time() - start_time:.2f}s")
        return answer, True

    def _generate_extractive(self, query: str, context: str) -> str:
        if not context.strip():
            return "No relevant information found in the document database."
        sentences = [s.strip() for s in context.replace('\n', ' ').split('.') if s.strip()]
        if not sentences:
            return "No relevant information found in the document database."
        return ". ".join(sentences[:3]) + "."

    def _build_prompt(self, query: str, context: str, tokenizer) -> str:
        messages = [
            {
                "role": "system",
                "content": (
                    "You are a helpful assistant that answers questions based on "
                    "the provided context. If the context doesn't contain relevant "
                    "information, say so honestly."
                ),
            },
            {
                "role": "user",
                "content": f"Context:\n{context}\n\nQuestion: {query}\n\nProvide a clear, concise answer based only on the context above.",
            },
        ]
        return tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )

    def unload_model(self, model_name: str):
        if model_name not in self.PIPELINE_REGISTRY:
            return
        try:
            del self.PIPELINE_REGISTRY[model_name]
            del self.MODEL_REGISTRY[model_name]
            del self.TOKENIZER_REGISTRY[model_name]
            self.PIPELINE_REGISTRY[model_name] = None
            self.MODEL_REGISTRY[model_name] = None
            self.TOKENIZER_REGISTRY[model_name] = None

            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            logger.info(f"Model '{model_name}' unloaded from memory")
        except Exception as e:
            logger.error(f"Error unloading model {model_name}: {e}", exc_info=True)


# Singleton instance
llm_service = LLMService()